# Masterarbeit: Interaktive Lernumgebung zu Large Language Models

## Anleitung zur Benutzung

Diese Lernumgebung besteht aus zwei getrennten Jupyter-Notebooks, die aufeinander aufbauen:  
1. **Notebook 1 ‚Äì Emotionserkennung mit Transformern**   ## üìò Notebook 1: Emotionserkennung mit Transformern  
[![√ñffnen in Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/FabianBuchheister/Transformer-Architektur-Unbox/HEAD?labpath=Notebook1.ipynb)

2. **Notebook 2 ‚Äì Die Vorverarbeitungskette einer Texteingabe in LLMs**  ## üìó Notebook 2: Die Vorverarbeitungskette einer Texteingabe in LLMs  
[![√ñffnen in Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/FabianBuchheister/Transformer-Architektur-Unbox/HEAD?labpath=Notebook2.ipynb)


Beide Notebooks sind **autonom und explorativ** konzipiert.  
Sie kombinieren instruktive und explorative Phasen, sind multimedial gestaltet und auf Oberstufensch√ºler:innen (Mathe-LK) sowie Lehramtsstudierende der Informatik ausgerichtet.

---

## Notebook 1: Emotionserkennung mit Transformern

### Didaktische Umrahmung

**Einleitungsgeschichte:**  
Die Lernenden starten mit einem Beispiel aus ihrem Alltag: Emotionserkennung durch Maschinen.  
Sie begegnen Modellen, die Gef√ºhle aus Text, Sprache oder Bildern erkennen k√∂nnen, und vergleichen diese mit der menschlichen Wahrnehmung.  
So wird Neugier geweckt und zugleich eine Br√ºcke zu ethischen Fragen geschlagen.

**Reflexion & kognitive Belastung:**  
Am Ende des Notebooks reflektieren die Lernenden ihren Erkenntnisprozess.  
Sie analysieren, welche Darstellungsformen das Verst√§ndnis unterst√ºtzt haben und wie sich Explorations- und Instruktionsphasen auf ihre kognitive Beanspruchung ausgewirkt haben.

---

### **Block 1 ‚Äì Explorationsaufgaben zur Emotionserkennung**

**Kurzbeschreibung:**  
Lernende erkunden drei Modelle zur Emotionserkennung (Text, Audio, Bild), vergleichen Mensch vs. Maschine und leiten erste Ideen zum Mechanismus der Erkennung ab.

**Lernziele:**  
- Modelle zur Emotionserkennung praktisch erproben und Ergebnisse reflektieren.  
- Erste konzeptionelle Br√ºcke zum zugrunde liegenden Mechanismus schlagen (Vorwissen Richtung Self-Attention).

**Grundvorstellungen:**  
- Emotionserkennung ist modalit√§ts√ºbergreifend (Text/Audio/Bild) m√∂glich und liefert auswertbare Klassifikationen.  
- Erste, vorl√§ufige Vorstellung: Modellentscheidungen beruhen auf systematischen Merkmalen der Eingabe (Ausblick auf Self-Attention).

---

### **Block 2 ‚Äì Ethische Betrachtung der Emotionserkennung**

**Kurzbeschreibung:**  
Diskussion von vier realit√§tsnahen F√§llen mit drei klassischen Ethiken (Utilitarismus, Deontologie, Tugendethik); Ergebnisse werden festgehalten.

**Lernziele (1:1 aus Masterarbeit):**  
Lernende verstehen die Anwendungsm√∂glichkeiten von Transformern im Rahmen der Emotionserkennung und reflektieren deren gesellschaftliche und ethische Implikationen anhand drei klassischer Ethiken.

**Grundvorstellungen:**  
- Bewertung von KI-Einsatz erfordert normatives Abw√§gen (Nutzen, Pflichten/Regeln, Tugenden).  
- Technikfolgen sind kontextabh√§ngig (Schule, Bewerbung, Unterricht, Kasse) und m√ºssen argumentativ begr√ºndet werden.

---

### **√úbergreifendes Notebook-Ziel (f√ºr N1)**  
Lernende verstehen die Anwendungsm√∂glichkeiten von Transformern im Rahmen der Emotionserkennung und reflektieren deren gesellschaftliche und ethische Implikationen anhand dreier klassischer Ethiken.

---

## Notebook 2: Die Vorverarbeitungskette einer Texteingabe in LLMs

### Didaktische Umrahmung

**Einleitungsgeschichte:**  
Die Lernenden begleiten eine Texteingabe auf ihrem Weg durch ein Sprachmodell ‚Äì von Buchstaben √ºber Tokens bis hin zu Vektoren.  
So wird der scheinbar ‚Äûmagische‚Äú Vorgang der Sprachverarbeitung Schritt f√ºr Schritt transparent gemacht.

**Reflexion & kognitive Belastung:**  
Am Ende reflektieren die Lernenden, welche Teilprozesse sie am st√§rksten gefordert haben, und bewerten den eigenen Lernfortschritt.  
Dadurch wird die metakognitive Bewusstheit gest√§rkt und √úberforderung reduziert.

---

### **Block 1 ‚Äì Exploration der Vokabeltabelle**

**Kurzbeschreibung:**  
Untersuchung des Token-Vokabulars; Subword-Strukturen werden sichtbar gemacht.

**Lernziele (1:1 aus Masterarbeit):**  
Lernende vollziehen den Prozess der Tokenisierung nach, indem sie:  
- das Konzept des Tokens als kleinste Verarbeitungseinheit und das der Vokabeltabelle eines Transformers explorativ erfahren.

**Grundvorstellungen (1:1 aus Masterarbeit ‚Äì Tokens):**  
- Tokens sind die kleinsten Verarbeitungseinheiten eines LLMs, in die Eingabew√∂rter aufgespalten werden.  
- Sie sind Bestandteil eines Vokabulars.  
- `##`-Tokens sind dabei St√ºcke eines Wortes, die nicht am Wortanfang stehen.  
- Die Sondertokens `[CLS]`, `[SEP]` und `[PAD]` strukturieren den Eingabesatz.  
- Der `[UNK]`-Token dient als Platzhalter f√ºr zu lange Eingabew√∂rter und f√ºr unbekannte Symbole.  
- Tokens sind eindeutigen IDs zugeordnet.

---

### **Block 2 ‚Äì Allgemeine Tokenisierung**

**Kurzbeschreibung:**  
Schrittweises Nachvollziehen der Tokenisierung mit realem deutschen BERT-Modell.

**Lernziele (1:1 aus Masterarbeit):**  
Lernende vollziehen den Prozess der Tokenisierung nach, indem sie:  
- die Tokenisierung anhand eines realen deutschen BERT-Modells eigenst√§ndig durchf√ºhren.

**Grundvorstellungen (1:1 aus Masterarbeit ‚Äì Tokenisierung):**  
- Lernende verstehen, dass die Tokenisierung der erste Schritt der Verarbeitungskette einer Texteingabe in einem LLM ist.  
- Sie wird durch einen Tokenisierer durchgef√ºhrt.  
- Es findet ein Abgleich mit einem Token-Vokabular statt.  
- Ist ein Wort nicht vorhanden, wird es so lange iterativ aufgespalten, bis das gesamte Wort gest√ºckelt wurde.

---

### **Block 2.1 ‚Äì WordPiece / Greedy-Longest-Match-First**

**Kurzbeschreibung:**  
Vertiefter Einblick in den Algorithmus der WordPiece-Tokenisierung nach Google Inc. (2018).  
Lernende durchlaufen den Original-Code, wenden das Verfahren auf eigene Eingaben an und reflektieren Sonder- und Randf√§lle.

**Lernziele (1:1 aus Masterarbeit):**  
Lernende vollziehen das Greedy-Longest-Match-First-Verfahren des WordPiece-Modells nach, indem sie:  
- den Original-Code der Google Inc. durchlaufen.  
- das Verfahren auf einen Eingabesatz ihrer Wahl anwenden.  
- Sonder- und Randf√§lle reflektieren.

**Grundvorstellungen (1:1 aus Masterarbeit):**  
- Ein h√§ufig verwendeter Algorithmus der Tokenisierung ist Greedy-Longest-Match-First im WordPiece-Modell der Google Inc. (2018).  
- Es handelt sich um ein heuristisches Suchverfahren, das von links nach rechts nach dem l√§ngsten, im Vokabular vorhandenen Wortst√ºck des Eingabewortes sucht.  
- Nach dem Finden eines Tokens spaltet es den Rest iterativ erneut auf.  
- Beispiele f√ºr Sonder- und Randf√§lle ‚Äì f√ºr die der `[UNK]`-Token reserviert ist ‚Äì werden reflektiert.

---

### **Block 3 ‚Äì Grundlagen der Vektorrechnung**

**Kurzbeschreibung:**  
Wiederholung zentraler Operationen (Betrag, Skalarprodukt, Vektoraddition) als Grundlage f√ºr Worteinbettungen.

**Lernziele (1:1 aus Masterarbeit):**  
Lernende beherrschen die Grundlagen der Vektorrechnung. Sie k√∂nnen:  
- Elemente eines Vektors ablesen.  
- Vektoren addieren und subtrahieren.  
- zweidimensionale Vektoren in ein Koordinatensystem einzeichnen.  
- den Betrag eines Vektors ermitteln.  
- das Skalarprodukt zweier Vektoren ermitteln.

*Hinweis:* _Zu diesem Lernblock wurden noch keine expliziten Grundvorstellungen formuliert._  
_Potentielle Herleitung:_ Lernende begreifen Vektoren als abstrakte Repr√§sentationen, auf die algebraische und geometrische Operationen angewendet werden k√∂nnen.

---

### **Block 4 ‚Äì Worteinbettungen (Embeddings)**

**Kurzbeschreibung:**  
Von Tokens zu Vektoren ‚Äì Repr√§sentation im semantischen Raum als Grundlage weiterer Verarbeitung.

**Lernziele (1:1 aus Masterarbeit):**  
Lernende verstehen das Konzept der Worteinbettungen auf mehreren Ebenen und k√∂nnen es erl√§utern, indem sie:  
- rezitieren, dass eine Einbettung ein hochdimensionaler Vektor in Zeilenvektor-Schreibweise ist, auf dem Rechenoperationen ausgef√ºhrt werden k√∂nnen.  
- erkl√§ren, wie aus einer Vokabeltabelle mithilfe der Token-ID eine Einbettung abgelesen wird.  
- reflektieren und beispielhaft erkl√§ren, dass Einbettungen als mentale Repr√§sentationen von Tokens und W√∂rtern fungieren und semantische wie auch grammatikalische Zusammenh√§nge kodieren.  
- reale Worteinbettungen im Tensor-Format explorieren.  
- ihr Wissen zu Einbettungen auf den nachfolgenden Lernblock zur Kosinus√§hnlichkeit √ºbertragen.

**Grundvorstellungen (1:1 aus Masterarbeit):**  
- (Wort-)Einbettungen sind hochdimensionale Vektoren im Spaltenvektor-Format.  
- Sie speichern semantische und syntaktische Eigenschaften eines Tokens.  
- Jedem Token des Vokabulars ist eine feste, eindeutige Vektorrepr√§sentation zugeordnet.  
- Zwischen Worteinbettungen bestehen √Ñhnlichkeitsbeziehungen, die ihre semantische N√§he widerspiegeln.  
- Semantische N√§he kann sich beispielsweise darin ausdr√ºcken, dass sich W√∂rter taxonomisch, thematisch oder grammatikalisch nahestehen oder h√§ufig im gleichen Kontext vorkommen.

---

### **Block 5 ‚Äì Kosinus-√Ñhnlichkeit & semantische N√§he**

**Kurzbeschreibung:**  
Messen semantischer N√§he zwischen Wortvektoren √ºber die Kosinus-√Ñhnlichkeit.

**Lernziele (1:1 aus Masterarbeit):**  
- Kosinus-√Ñhnlichkeit berechnen und interpretieren.  
- Semantische N√§he anhand geometrischer und algebraischer Zug√§nge diskutieren.

**Grundvorstellungen (1:1 aus Masterarbeit):**  
- Die Kosinus√§hnlichkeit beschreibt die √Ñhnlichkeit zweier Einbettungsvektoren ausschlie√ülich √ºber ihren Winkel zueinander ‚Äì unabh√§ngig von ihrer L√§nge.  
- Sie stellt ein legitimes und anschauliches Werkzeug dar, um Bedeutungsn√§he visuell zu explorieren.

---

### **Block 6 ‚Äì Positional Encoding**

**Kurzbeschreibung:**  
Positionsinformation wird als Zusatzsignal eingebracht, um Reihenfolge zu kodieren.

*Hinweis:* _Zu diesem Lernblock wurden noch keine Grundvorstellungen formuliert._  
_Potentielle Herleitung:_ Lernende verstehen, dass Bedeutung auch durch Reihenfolge entsteht und Positionsinformationen notwendig sind, um Satzstruktur abzubilden.

---

### **Block 7 ‚Äì Quiz, Worked Example & Transferaufgabe**

**Kurzbeschreibung:**  
√úberpr√ºfen, festigen und √ºbertragen: angeleitetes Beispiel plus eigenst√§ndige Anwendung.

*Hinweis:* _Lernziele und Grundvorstellungen wurden im Ergebnisteil nicht konkret formuliert._  
_Potentielle Herleitung:_ Lernende wenden Konzepte der Tokenisierung und Einbettung eigenst√§ndig auf neue S√§tze an, um Transferf√§higkeit zu sichern.

---

### **Block 8 ‚Äì Reflexion & kognitive Belastung**

**Kurzbeschreibung:**  
Selbstreflexion zu Lernfortschritt und Beanspruchung; Passung der Instruktions-/Explorationsanteile.

**Lernziele:**  
- Eigenen Lernprozess bewerten und Strategien anpassen.

**Grundvorstellungen:**  
- Multimediale, explorative Elemente und klare Instruktionsschritte wirken gemeinsam lernwirksam.  
- Transparenz √ºber Mechanismen erh√∂ht Nachvollziehbarkeit und reduziert kognitive Belastung.

---

## √úbergreifendes Ziel (f√ºr N2)

Lernende verstehen die komplette Vorverarbeitungskette einer Texteingabe ‚Äì von der Tokenisierung bis zur semantischen √Ñhnlichkeitsmessung ‚Äì und k√∂nnen diese auf reale LLMs √ºbertragen.
